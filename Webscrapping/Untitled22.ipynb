{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c87cbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Header Tags\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you knowÂ ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get the header tags from a URL\n",
    "def get_header_tags(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    return [tag.text.strip() for tag in header_tags]\n",
    "\n",
    "# Wikipedia URL\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "# Get header tags\n",
    "header_tags = get_header_tags(wiki_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Header Tags': header_tags})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf06bc37",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://presidentofindia.nic.in/former-presidents.htm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Get the list of former presidents\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m former_presidents_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_former_presidents\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(former_presidents_list)\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mget_former_presidents\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract information for each president\u001b[39;00m\n\u001b[0;32m     14\u001b[0m presidents_info \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m president_entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mformer_presidents_section\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     16\u001b[0m     name \u001b[38;5;241m=\u001b[39m president_entry\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrong\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m     term_of_office \u001b[38;5;241m=\u001b[39m president_entry\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get the list of former presidents from the specified URL\n",
    "def get_former_presidents(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the section containing the list of former presidents\n",
    "    former_presidents_section = soup.find('div', {'class': 'president_lst'})\n",
    "\n",
    "    # Extract information for each president\n",
    "    presidents_info = []\n",
    "    for president_entry in former_presidents_section.find_all('li'):\n",
    "        name = president_entry.find('strong').text.strip()\n",
    "        term_of_office = president_entry.find('span').text.strip()\n",
    "        presidents_info.append({'Name': name, 'Term of Office': term_of_office})\n",
    "\n",
    "    return presidents_info\n",
    "\n",
    "# URL of the website\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "\n",
    "# Get the list of former presidents\n",
    "former_presidents_list = get_former_presidents(url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(former_presidents_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15870df2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m odi_rankings_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.icc-cricket.com/rankings/team-rankings/mens/odi\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Get the ODI rankings\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m odi_rankings_list \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_odi_rankings\u001b[49m\u001b[43m(\u001b[49m\u001b[43modi_rankings_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     33\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(odi_rankings_list)\n",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m, in \u001b[0;36mscrape_odi_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract information for each team\u001b[39;00m\n\u001b[0;32m     14\u001b[0m teams_info \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m11\u001b[39m]:  \u001b[38;5;66;03m# Skip the header row and take top 10 teams\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     team_name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape ICC ODI rankings and create a DataFrame\n",
    "def scrape_odi_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing ODI rankings\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Extract information for each team\n",
    "    teams_info = []\n",
    "    for row in table.find_all('tr')[1:11]:  # Skip the header row and take top 10 teams\n",
    "        columns = row.find_all('td')\n",
    "        team_name = columns[1].text.strip()\n",
    "        matches = columns[2].text.strip()\n",
    "        points = columns[3].text.strip()\n",
    "        rating = columns[4].text.strip()\n",
    "\n",
    "        teams_info.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    return teams_info\n",
    "\n",
    "# URL of the ICC ODI rankings for men's cricket\n",
    "odi_rankings_url = 'https://www.icc-cricket.com/rankings/team-rankings/mens/odi'\n",
    "\n",
    "# Get the ODI rankings\n",
    "odi_rankings_list = scrape_odi_rankings(odi_rankings_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(odi_rankings_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a6125b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m odi_batsmen_rankings_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.icc-cricket.com/rankings/mens/player-rankings/test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Get the ODI batsmen rankings\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m odi_batsmen_rankings_list \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_odi_batsmen_rankings\u001b[49m\u001b[43m(\u001b[49m\u001b[43modi_batsmen_rankings_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     33\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(odi_batsmen_rankings_list)\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mscrape_odi_batsmen_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract information for each batsman\u001b[39;00m\n\u001b[0;32m     14\u001b[0m batsmen_info \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m11\u001b[39m]:  \u001b[38;5;66;03m# Skip the header row and take top 10 batsmen\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     position \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape ICC ODI batsmen rankings and create a DataFrame\n",
    "def scrape_odi_batsmen_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing ODI batsmen rankings\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Extract information for each batsman\n",
    "    batsmen_info = []\n",
    "    for row in table.find_all('tr')[1:11]:  # Skip the header row and take top 10 batsmen\n",
    "        columns = row.find_all('td')\n",
    "        position = columns[0].text.strip()\n",
    "        batsman_name = columns[1].text.strip()\n",
    "        team_name = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        batsmen_info.append({'Position': position, 'Batsman': batsman_name, 'Team': team_name, 'Rating': rating})\n",
    "\n",
    "    return batsmen_info\n",
    "\n",
    "# URL of the ICC ODI batsmen rankings\n",
    "odi_batsmen_rankings_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/test'\n",
    "\n",
    "# Get the ODI batsmen rankings\n",
    "odi_batsmen_rankings_list = scrape_odi_batsmen_rankings(odi_batsmen_rankings_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(odi_batsmen_rankings_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1164fb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m womens_odi_rankings_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.icc-cricket.com/rankings/womens/team-rankings/odi\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Get the Women's ODI rankings\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m womens_odi_rankings_list \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_womens_odi_rankings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwomens_odi_rankings_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     33\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(womens_odi_rankings_list)\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mscrape_womens_odi_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract information for each team\u001b[39;00m\n\u001b[0;32m     14\u001b[0m teams_info \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m11\u001b[39m]:  \u001b[38;5;66;03m# Skip the header row and take top 10 teams\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     team_name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape ICC Women's ODI rankings and create a DataFrame\n",
    "def scrape_womens_odi_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing Women's ODI rankings\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Extract information for each team\n",
    "    teams_info = []\n",
    "    for row in table.find_all('tr')[1:11]:  # Skip the header row and take top 10 teams\n",
    "        columns = row.find_all('td')\n",
    "        team_name = columns[1].text.strip()\n",
    "        matches = columns[2].text.strip()\n",
    "        points = columns[3].text.strip()\n",
    "        rating = columns[4].text.strip()\n",
    "\n",
    "        teams_info.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "\n",
    "    return teams_info\n",
    "\n",
    "# URL of the ICC Women's ODI rankings\n",
    "womens_odi_rankings_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "\n",
    "# Get the Women's ODI rankings\n",
    "womens_odi_rankings_list = scrape_womens_odi_rankings(womens_odi_rankings_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(womens_odi_rankings_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64a5e205",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m womens_odi_batting_rankings_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Get the Women's ODI batting rankings\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m womens_odi_batting_rankings_list \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_womens_odi_batting_rankings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwomens_odi_batting_rankings_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     33\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(womens_odi_batting_rankings_list)\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mscrape_womens_odi_batting_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract information for each batsman\u001b[39;00m\n\u001b[0;32m     14\u001b[0m batsmen_info \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m11\u001b[39m]:  \u001b[38;5;66;03m# Skip the header row and take top 10 batsmen\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     position \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape ICC Women's ODI batting rankings and create a DataFrame\n",
    "def scrape_womens_odi_batting_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing Women's ODI batting rankings\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Extract information for each batsman\n",
    "    batsmen_info = []\n",
    "    for row in table.find_all('tr')[1:11]:  # Skip the header row and take top 10 batsmen\n",
    "        columns = row.find_all('td')\n",
    "        position = columns[0].text.strip()\n",
    "        batsman_name = columns[1].text.strip()\n",
    "        team_name = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "\n",
    "        batsmen_info.append({'Position': position, 'Batsman': batsman_name, 'Team': team_name, 'Rating': rating})\n",
    "\n",
    "    return batsmen_info\n",
    "\n",
    "# URL of the ICC Women's ODI batting rankings\n",
    "womens_odi_batting_rankings_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "\n",
    "# Get the Women's ODI batting rankings\n",
    "womens_odi_batting_rankings_list = scrape_womens_odi_batting_rankings(womens_odi_batting_rankings_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(womens_odi_batting_rankings_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e5d291",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m cnbc_world_news_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cnbc.com/world/?region=world\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Get the news details\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m cnbc_news_list \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_cnbc_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnbc_world_news_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     31\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cnbc_news_list)\n",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m, in \u001b[0;36mscrape_cnbc_news\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract information for each news article\u001b[39;00m\n\u001b[0;32m     14\u001b[0m news_info \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m \u001b[43marticles_section\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCard-titleContainer\u001b[39m\u001b[38;5;124m'\u001b[39m}):\n\u001b[0;32m     16\u001b[0m     headline \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m     time \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape news details and create a DataFrame\n",
    "def scrape_cnbc_news(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the section containing news articles\n",
    "    articles_section = soup.find('div', {'id': 'main-container'})\n",
    "\n",
    "    # Extract information for each news article\n",
    "    news_info = []\n",
    "    for article in articles_section.find_all('div', {'class': 'Card-titleContainer'}):\n",
    "        headline = article.find('a').text.strip()\n",
    "        time = article.find('time')['datetime']\n",
    "        news_link = 'https://www.cnbc.com' + article.find('a')['href']\n",
    "\n",
    "        news_info.append({'Headline': headline, 'Time': time, 'News Link': news_link})\n",
    "\n",
    "    return news_info\n",
    "\n",
    "# URL of the CNBC World News page\n",
    "cnbc_world_news_url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Get the news details\n",
    "cnbc_news_list = scrape_cnbc_news(cnbc_world_news_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(cnbc_news_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed054a1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m most_downloaded_articles_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Get the most downloaded articles details\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m most_downloaded_articles_list \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_most_downloaded_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmost_downloaded_articles_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     32\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(most_downloaded_articles_list)\n",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m, in \u001b[0;36mscrape_most_downloaded_articles\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract information for each article\u001b[39;00m\n\u001b[0;32m     14\u001b[0m articles_info \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m \u001b[43marticles_section\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpod-listing\u001b[39m\u001b[38;5;124m'\u001b[39m}):\n\u001b[0;32m     16\u001b[0m     title \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh4\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m     authors \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjs-article-item-authors\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape most downloaded articles details and create a DataFrame\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the section containing most downloaded articles\n",
    "    articles_section = soup.find('ul', {'class': 'list-unstyled'})\n",
    "\n",
    "    # Extract information for each article\n",
    "    articles_info = []\n",
    "    for article in articles_section.find_all('li', {'class': 'pod-listing'}):\n",
    "        title = article.find('h4').text.strip()\n",
    "        authors = article.find('p', {'class': 'js-article-item-authors'}).text.strip()\n",
    "        published_date = article.find('p', {'class': 'text-s font-m margin-bottom-xxs js-article-item-pub-date'}).text.strip()\n",
    "        paper_url = 'https://www.journals.elsevier.com' + article.find('a', {'class': 'pod-listing-header-link'})['href']\n",
    "\n",
    "        articles_info.append({'Paper Title': title, 'Authors': authors, 'Published Date': published_date, 'Paper URL': paper_url})\n",
    "\n",
    "    return articles_info\n",
    "\n",
    "# URL of the most downloaded articles page\n",
    "most_downloaded_articles_url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "\n",
    "# Get the most downloaded articles details\n",
    "most_downloaded_articles_list = scrape_most_downloaded_articles(most_downloaded_articles_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(most_downloaded_articles_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b411b00a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m dineout_restaurants_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.dineout.co.in/delhi-restaurants\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Get the restaurant details\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m dineout_restaurants_list \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_dineout_restaurants\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdineout_restaurants_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     33\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dineout_restaurants_list)\n",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m, in \u001b[0;36mscrape_dineout_restaurants\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract information for each restaurant\u001b[39;00m\n\u001b[0;32m     14\u001b[0m restaurants_info \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m restaurant \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrestaurants_section\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrestnt-info\u001b[39m\u001b[38;5;124m'\u001b[39m}):\n\u001b[0;32m     16\u001b[0m     name \u001b[38;5;241m=\u001b[39m restaurant\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrestnt-name\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m     cuisine \u001b[38;5;241m=\u001b[39m restaurant\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble-line\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape restaurant details and create a DataFrame\n",
    "def scrape_dineout_restaurants(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the section containing restaurant details\n",
    "    restaurants_section = soup.find('ul', {'class': 'restaurant-list'})\n",
    "\n",
    "    # Extract information for each restaurant\n",
    "    restaurants_info = []\n",
    "    for restaurant in restaurants_section.find_all('div', {'class': 'restnt-info'}):\n",
    "        name = restaurant.find('a', {'class': 'restnt-name'}).text.strip()\n",
    "        cuisine = restaurant.find('span', {'class': 'double-line'}).text.strip()\n",
    "        location = restaurant.find('span', {'class': 'locality'}).text.strip()\n",
    "        ratings = restaurant.find('div', {'class': 'rating'}).text.strip()\n",
    "        image_url = restaurant.find('img')['src']\n",
    "\n",
    "        restaurants_info.append({'Restaurant Name': name, 'Cuisine': cuisine, 'Location': location, 'Ratings': ratings, 'Image URL': image_url})\n",
    "\n",
    "    return restaurants_info\n",
    "\n",
    "# URL of the dineout restaurants page\n",
    "dineout_restaurants_url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "\n",
    "# Get the restaurant details\n",
    "dineout_restaurants_list = scrape_dineout_restaurants(dineout_restaurants_url)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(dineout_restaurants_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb6ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9720289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
